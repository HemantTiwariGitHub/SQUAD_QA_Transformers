{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Question_Answering_with_SQuAD_2_0_20210102.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HemantTiwariGitHub/CapstoneProject2021/blob/main/Question_Answering_with_SQuAD_2_0_20210102.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Tz7wt7G_kLd"
      },
      "source": [
        "**Question answering** comes in many forms. In this example, weâ€™ll look at the particular type of extractive QA that involves answering a question about a passage by highlighting the segment of the passage that answers the question. This involves fine-tuning a model which predicts a start position and an end position in the passage. We will use the Stanford Question Answering Dataset (SQuAD) 2.0.\n",
        "\n",
        "We will start by downloading the data:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKgPO4AUDZYi"
      },
      "source": [
        "## **Note :**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAv2_F8VDL9t"
      },
      "source": [
        "Please write your code in the cells with the \"**Your code here**\" placeholder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ockcf0NvAHy5"
      },
      "source": [
        "## **Download SQuAD 2.0 Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pmSMjQN_5zd"
      },
      "source": [
        "Note : This dataset can be explored in the Hugging Face model hub (SQuAD V2), and can be alternatively downloaded with the ðŸ¤— NLP library with load_dataset(\"squad_v2\")."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GlKOoe6pLmQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9d15969-cf48-4594-ec75-9783c0751a0c"
      },
      "source": [
        "!mkdir squad\n",
        "!wget https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json -O /content/squad/train-v2.0.json\n",
        "!wget https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json -O /content/squad/dev-v2.0.json"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-01-02 08:12:50--  https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\n",
            "Resolving rajpurkar.github.io (rajpurkar.github.io)... 185.199.108.153, 185.199.109.153, 185.199.110.153, ...\n",
            "Connecting to rajpurkar.github.io (rajpurkar.github.io)|185.199.108.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 42123633 (40M) [application/json]\n",
            "Saving to: â€˜/content/squad/train-v2.0.jsonâ€™\n",
            "\n",
            "/content/squad/trai 100%[===================>]  40.17M   134MB/s    in 0.3s    \n",
            "\n",
            "2021-01-02 08:12:51 (134 MB/s) - â€˜/content/squad/train-v2.0.jsonâ€™ saved [42123633/42123633]\n",
            "\n",
            "--2021-01-02 08:12:51--  https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json\n",
            "Resolving rajpurkar.github.io (rajpurkar.github.io)... 185.199.108.153, 185.199.109.153, 185.199.111.153, ...\n",
            "Connecting to rajpurkar.github.io (rajpurkar.github.io)|185.199.108.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4370528 (4.2M) [application/json]\n",
            "Saving to: â€˜/content/squad/dev-v2.0.jsonâ€™\n",
            "\n",
            "/content/squad/dev- 100%[===================>]   4.17M  23.2MB/s    in 0.2s    \n",
            "\n",
            "2021-01-02 08:12:51 (23.2 MB/s) - â€˜/content/squad/dev-v2.0.jsonâ€™ saved [4370528/4370528]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSu1qWXGbZrl",
        "outputId": "9143b142-513b-4226-bb9f-92a6b397b70f"
      },
      "source": [
        "!pip install tqdm\n",
        "import tqdm"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.41.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6iUxzrWhZVyI"
      },
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "def loadJSONData(filename):\n",
        "    with open(filename) as jsonDataFile:\n",
        "        data = json.load(jsonDataFile)\n",
        "    return data"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppGG00nMamo_"
      },
      "source": [
        "#Data has Multiple Titles\n",
        "#Every Title has Multiple Paragraphs and Each Para has Text in Context\n",
        "#Every Paragraphs has Multiple Questions and Every Question has multiple answers with Answer start index\n",
        "#If Answer is plausible , is_impossible is False.\n",
        "\n",
        "def preprocessSQUAD(JSONData):\n",
        "    lengthOfData = len(JSONData['data'])\n",
        "    BaseData = JSONData['data']\n",
        "    print(\"Length Of JSON Data : \", lengthOfData)\n",
        "    for titleID in (range(2)):\n",
        "      title = BaseData[titleID]['title']\n",
        "      paragraphs = BaseData[titleID]['paragraphs']\n",
        "      paragraphCount = len(paragraphs)\n",
        "\n",
        "      for (paraID in range(paragraphCount)):\n",
        "        context = paragraphs[paraID]\n",
        "        \n",
        "\n",
        "\n",
        "      context = paragraphs[0]['context']\n",
        "      print (titleID, title,paragraphCount,context)\n",
        "\n"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XP1dUfMdeYnp",
        "outputId": "6c1ab6a9-a05f-45f7-857e-20cbcb75cf82"
      },
      "source": [
        "read_squad('/content/squad/dev-v2.0.json')"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length Of JSON Data :  35\n",
            "0 Normans 39 The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.\n",
            "1 Computational_complexity_theory 48 Computational complexity theory is a branch of the theory of computation in theoretical computer science that focuses on classifying computational problems according to their inherent difficulty, and relating those classes to each other. A computational problem is understood to be a task that is in principle amenable to being solved by a computer, which is equivalent to stating that the problem may be solved by mechanical application of mathematical steps, such as an algorithm.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 135
        },
        "id": "GK-BhYUDYG2O",
        "outputId": "82a59913-8c39-4f54-b096-a10f3a9f1dbd"
      },
      "source": [
        "def preprocessSQUAD1(JSONData):\n",
        "    num_mappingprob, num_tokenprob, num_spanalignprob = 0, 0, 0\n",
        "    examples = []\n",
        "    print(len(JSONData['data'])\n",
        "\n",
        "    for articles_id in tqdm(range(len(JSONData['data']))):\n",
        "\n",
        "        article_paragraphs = JSONData['data'][articles_id]['paragraphs']\n",
        "        for pid in range(len(article_paragraphs)):\n",
        "\n",
        "            context = unicode(article_paragraphs[pid]['context'])\n",
        "            context = context.replace(\"''\", '\" ')\n",
        "            context = context.replace(\"``\", '\" ')\n",
        "\n",
        "            context_tokens = tokenize(context) # list of strings (lowercase)\n",
        "            context = context.lower()\n",
        "\n",
        "            qas = article_paragraphs[pid]['qas'] # list of questions\n",
        "\n",
        "            charloc2wordloc = get_char_word_loc_mapping(context, context_tokens) # charloc2wordloc maps the character location (int) of a context token to a pair giving (word (string), word loc (int)) of that token\n",
        "\n",
        "            if charloc2wordloc is None: # there was a problem\n",
        "                num_mappingprob += len(qas)\n",
        "                continue # skip this context example\n",
        "\n",
        "            # for each question, process the question and answer and write to file\n",
        "            for qn in qas:\n",
        "\n",
        "                # read the question text and tokenize\n",
        "                question = unicode(qn['question']) # string\n",
        "                question_tokens = tokenize(question) # list of strings\n",
        "\n",
        "                # of the three answers, just take the first\n",
        "                ans_text = unicode(qn['answers'][0]['text']).lower() # get the answer text\n",
        "                ans_start_charloc = qn['answers'][0]['answer_start'] # answer start loc (character count)\n",
        "                ans_end_charloc = ans_start_charloc + len(ans_text) # answer end loc (character count) (exclusive)\n",
        "\n",
        "                # Check that the provided character spans match the provided answer text\n",
        "                if context[ans_start_charloc:ans_end_charloc] != ans_text:\n",
        "                  # Sometimes this is misaligned, mostly because \"narrow builds\" of Python 2 interpret certain Unicode characters to have length 2 https://stackoverflow.com/questions/29109944/python-returns-length-of-2-for-single-unicode-character-string\n",
        "                  # We should upgrade to Python 3 next year!\n",
        "                  num_spanalignprob += 1\n",
        "                  continue\n",
        "\n",
        "                # get word locs for answer start and end (inclusive)\n",
        "                ans_start_wordloc = charloc2wordloc[ans_start_charloc][1] # answer start word loc\n",
        "                ans_end_wordloc = charloc2wordloc[ans_end_charloc-1][1] # answer end word loc\n",
        "                assert ans_start_wordloc <= ans_end_wordloc\n",
        "\n",
        "                # Check retrieved answer tokens match the provided answer text.\n",
        "                # Sometimes they won't match, e.g. if the context contains the phrase \"fifth-generation\"\n",
        "                # and the answer character span is around \"generation\",\n",
        "                # but the tokenizer regards \"fifth-generation\" as a single token.\n",
        "                # Then ans_tokens has \"fifth-generation\" but the ans_text is \"generation\", which doesn't match.\n",
        "                ans_tokens = context_tokens[ans_start_wordloc:ans_end_wordloc+1]\n",
        "                if \"\".join(ans_tokens) != \"\".join(ans_text.split()):\n",
        "                    num_tokenprob += 1\n",
        "                    continue # skip this question/answer pair\n",
        "\n",
        "                examples.append((' '.join(context_tokens), ' '.join(question_tokens), ' '.join(ans_tokens), ' '.join([str(ans_start_wordloc), str(ans_end_wordloc)])))\n",
        "\n",
        "                num_exs += 1\n",
        "\n",
        "    #print \"Number of (context, question, answer) triples discarded due to char -> token mapping problems: \", num_mappingprob\n",
        "    #print \"Number of (context, question, answer) triples discarded because character-based answer span is unaligned with tokenization: \", num_tokenprob\n",
        "    #print \"Number of (context, question, answer) triples discarded due character span alignment problems (usually Unicode problems): \", num_spanalignprob\n",
        "    #print \"Processed %i examples of total %i\\n\" % (num_exs, num_exs + num_mappingprob + num_tokenprob + num_spanalignprob)\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-37-8e3da982808a>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    for articles_id in tqdm(range(len(JSONData['data']))):\u001b[0m\n\u001b[0m                                                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Jv7ipy5AYUM"
      },
      "source": [
        "Each split is in a structured json file with a number of questions and answers for each passage (or context). Weâ€™ll take this apart into parallel lists of contexts, questions, and answers (note that the contexts here are repeated since there are multiple questions per context):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKMDLA8woiA8"
      },
      "source": [
        "def read_squad(path):\n",
        "  dataInJSON = loadJSONData(path)\n",
        "  preprocessSQUAD(dataInJSON)\n",
        "\n",
        "  # Your code here\n",
        "  \n",
        "  #return contexts, questions, answers\n",
        "\n",
        "#train_contexts, train_questions, train_answers = read_squad('/content/squad/train-v2.0.json')\n",
        "#val_contexts, val_questions, val_answers = read_squad('/content/squad/dev-v2.0.json')\n"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83J5neKoYFbs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bPtDyaFAtQm"
      },
      "source": [
        "The contexts and questions are just strings. The answers are dicts containing the subsequence of the passage with the correct answer as well as an integer indicating the character at which the answer begins. In order to train a model on this data we need (1) the tokenized context/question pairs, and (2) integers indicating at which token positions the answer begins and ends.\n",
        "\n",
        "First, letâ€™s get the character position at which the answer ends in the passage (we are given the starting position). Sometimes SQuAD answers are off by one or two characters, so we will also adjust for that."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nraxKhioA1rG"
      },
      "source": [
        "def add_end_idx(answers, contexts):\n",
        "    for answer, context in zip(answers, contexts):\n",
        "      \n",
        "      # Your code here\n",
        "        ...\n",
        "\n",
        "add_end_idx(train_answers, train_contexts)\n",
        "add_end_idx(val_answers, val_contexts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmLsa8hQA2a7"
      },
      "source": [
        "Now train_answers and val_answers include the character end positions and the corrected start positions. Next, letâ€™s tokenize our context/question pairs. ðŸ¤— Tokenizers can accept parallel lists of sequences and encode them together as sequence pairs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdhZS6o0qMty"
      },
      "source": [
        "from transformers import DistilBertTokenizerFast\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "# Your code here\n",
        "train_encodings = ...\n",
        "\n",
        "# Your code here\n",
        "val_encodings = ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJdAikOIA_Zo"
      },
      "source": [
        "Next we need to convert our character start/end positions to token start/end positions. When using ðŸ¤— Fast Tokenizers, we can use the <b>built in char_to_token()</b> method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kau7PrtIBAde"
      },
      "source": [
        "def add_token_positions(encodings, answers):\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "    \n",
        "    # Your code here\n",
        "    ...\n",
        "\n",
        "    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
        "\n",
        "add_token_positions(train_encodings, train_answers)\n",
        "add_token_positions(val_encodings, val_answers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLIp2RhABITo"
      },
      "source": [
        "Our data is ready. Letâ€™s just put it in a PyTorch/TensorFlow dataset so that we can easily use it for training. In PyTorch, we define a custom Dataset class. In TensorFlow, we pass a tuple of (inputs_dict, labels_dict) to the from_tensor_slices method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cchIqVWpBI7X"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Your code here\n",
        "train_dataset = ...\n",
        "))\n",
        "\n",
        "# Your code here\n",
        "val_dataset = ...\n",
        "))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OshKaYppBOtD"
      },
      "source": [
        "Now we can use a DistilBert model with a QA head for training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sGzpUDvBPVU"
      },
      "source": [
        "from transformers import TFDistilBertForQuestionAnswering\n",
        "\n",
        "# Your code here\n",
        "model = ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UGXo8SwBTeX"
      },
      "source": [
        "The data and model are both ready to go. You can train the model with Trainer/TFTrainer exactly as in the sequence classification example above. If using native PyTorch, replace labels with start_positions and end_positions in the training example. If using Kerasâ€™s fit, we need to make a minor modification to handle this example since it involves multiple model outputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSwoxCD_BU_D"
      },
      "source": [
        "# Keras will expect a tuple when dealing with labels\n",
        "\n",
        "# Write your code here to replace labels with start_positions and end_positions in the training example\n",
        "train_dataset = train_dataset.map(...)\n",
        "\n",
        "# Keras will assign a separate loss for each output and add them together. So we'll just use the standard CE loss\n",
        "# instead of using the built-in model.compute_loss, which expects a dict of outputs and averages the two terms.\n",
        "# Note that this means the loss will be 2x of when using TFTrainer since we're adding instead of averaging them.\n",
        "\n",
        "# Your code here\n",
        "loss = ...\n",
        "model.distilbert.return_dict = False # if using ðŸ¤— Transformers >3.02, make sure outputs are tuples\n",
        "\n",
        "# Your code here\n",
        "optimizer = ...\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=loss) # can also use any keras loss fn\n",
        "model.fit(train_dataset.shuffle(1000).batch(16), epochs=3, batch_size=16)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}